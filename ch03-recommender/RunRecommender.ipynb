{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.conf\n",
    "import pyspark.sql\n",
    "SparkConf = pyspark.conf.SparkConf\n",
    "SparkSession = pyspark.sql.SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(\"Intro\") \\\n",
    "            .config('spark.executor.memory', '2g') \\\n",
    "            .config('spark.driver.memory','8g') \\\n",
    "            .config(\"spark.sql.crossJoin.enabled\", \"true\")\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/home/ubuntu/profiledata_06-May-2005/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = datadir\n",
    "rawUserArtistData = spark.read.text(base + \"user_artist_data.txt\")\n",
    "rawArtistData = spark.read.text(base + \"artist_data.txt\")\n",
    "rawArtistAlias = spark.read.text(base + \"artist_alias.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(value='1000002 1 55')\n",
      "Row(value='1000002 1000006 33')\n",
      "Row(value='1000002 1000007 8')\n",
      "Row(value='1000002 1000009 144')\n",
      "Row(value='1000002 1000010 314')\n"
     ]
    }
   ],
   "source": [
    "for _ in rawUserArtistData.take(5):\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fx(row):\n",
    "    cols = row.value.split(' ')\n",
    "    user, artist = cols[:2]\n",
    "    return int(user), int(artist)\n",
    "# end def\n",
    "    \n",
    "userArtistDF = rawUserArtistData.rdd.map(fx).toDF([\"user\", \"artist\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+-----------+\n",
      "|min(user)|max(user)|min(artist)|max(artist)|\n",
      "+---------+---------+-----------+-----------+\n",
      "|       90|  2443548|          1|   10794401|\n",
      "+---------+---------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userArtistDF.agg(F.min(\"user\"), F.max(\"user\"), F.min(\"artist\"), F.max(\"artist\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildArtistByID(rawArtistData):\n",
    "    def func(row):\n",
    "        try:\n",
    "            (_id, name) = row.value.split('\\t')\n",
    "        except ValueError:\n",
    "            return None, None\n",
    "        # end try\n",
    "        if (name.strip() == ''):\n",
    "            return None, None\n",
    "        else:\n",
    "            try:\n",
    "                return int(_id), name.strip()\n",
    "            except:\n",
    "                return None, None\n",
    "            # end try\n",
    "        # end if\n",
    "    # end def\n",
    "    return rawArtistData.rdd.map(func).toDF([\"id\", \"name\"]).na.drop()\n",
    "# end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildArtistAlias(rawArtistAlias):\n",
    "    def func(row):\n",
    "        try:\n",
    "            artist, alias = row.value.split('\\t')\n",
    "        except ValueError:\n",
    "            return None, None\n",
    "        # end try\n",
    "        if (artist.strip()==''):\n",
    "            return None, None\n",
    "        else:\n",
    "            return int(artist), int(alias)\n",
    "        # end if\n",
    "    # end def\n",
    "    return dict(rawArtistAlias.rdd.map(func).collect())\n",
    "# end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "artistByID = buildArtistByID(rawArtistData)\n",
    "artistAlias = buildArtistAlias(rawArtistAlias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(badID, goodID) = next(iter(artistAlias.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|     id|              name|\n",
      "+-------+------------------+\n",
      "|1109457|             P.O.S|\n",
      "|2097152|DJ Tiesto -  P.O.S|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artistByID.filter(F.col('id').isin([badID, goodID])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildCounts(rawUserArtistData, bArtistAlias):\n",
    "    def func(row):\n",
    "        try:\n",
    "            userID, artistID, count = map(int, row.value.split(' '))\n",
    "            finalArtistID = bArtistAlias.value.get(artistID, artistID)\n",
    "            return (userID, finalArtistID, count)\n",
    "        except ValueError:\n",
    "            return None, None, None\n",
    "        # end try\n",
    "    # end def\n",
    "    return rawUserArtistData.rdd.map(func).toDF([\"user\", \"artist\", \"count\"]).na.drop()\n",
    "# end def\n",
    "trainData = buildCounts(rawUserArtistData, bArtistAlias).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ALS().\\\n",
    "    setSeed(random.randrange(0,10000000)).\\\n",
    "    setImplicitPrefs(True).\\\n",
    "    setRank(10).\\\n",
    "    setRegParam(0.01).\\\n",
    "    setAlpha(1.0).\\\n",
    "    setMaxIter(5).\\\n",
    "    setUserCol(\"user\").\\\n",
    "    setItemCol(\"artist\").\\\n",
    "    setRatingCol(\"count\").\\\n",
    "    setPredictionCol(\"prediction\").\\\n",
    "    fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user: bigint, artist: bigint, count: bigint]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                  |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[-0.53729624, -0.028494425, -0.06313752, -0.19646008, -0.26367348, 0.023305353, 0.07086386, 0.24986394, 0.44049087, 1.3109902]            |\n",
      "|[-0.007965173, 0.06658514, -0.21245287, 0.16654468, -0.12804265, 0.21028997, -0.15430218, -0.020562911, 0.17293294, -0.13754143]          |\n",
      "|[0.003127225, 0.0029438583, 0.001785691, 8.5407676E-4, -0.0012625128, 0.0056536254, 0.0030825643, 9.866614E-4, -0.0012622245, 0.006635723]|\n",
      "|[-0.91178507, -0.062251084, -0.31099108, -0.73389816, -1.2645013, 0.14698508, -0.73243695, 0.15284924, 0.43211594, 1.0212367]             |\n",
      "|[-0.7790672, -0.7534829, -1.2736305, 0.13149153, -0.97712433, 0.88457495, -0.83778405, 0.1260009, 0.9207142, 0.22913049]                  |\n",
      "|[-1.4374225, 3.3804257, 0.2360429, 1.6931623, 0.6962605, 0.014771904, -2.0942912, 0.5628247, 0.34377962, -0.24999952]                     |\n",
      "|[-0.6327491, 0.27853617, -0.86760557, -0.050249062, -1.0362986, 0.5600739, -0.64437413, 0.3784949, 0.99679047, -0.079245076]              |\n",
      "|[-0.05242474, 0.022659365, -0.0067415624, 0.058291204, -0.065764345, -0.05934233, -0.07773094, -0.028633777, -0.04168192, 0.04040325]     |\n",
      "|[-0.86120874, 0.2367634, 0.35167822, 0.8561058, -1.0442975, 0.6825669, -0.2906478, -1.3640484, -0.46385857, -0.41892877]                  |\n",
      "|[-0.16129379, -0.29900742, -0.27608827, 0.41623747, -0.5996947, 1.158167, -0.6763868, 0.501845, 0.05254605, -0.2155113]                   |\n",
      "|[-1.7279497, -0.3330092, -0.9997334, 0.56347334, -1.7324996, 0.7172074, -1.7440585, 0.21255584, 0.43051007, 2.3203204]                    |\n",
      "|[-1.4071503, -0.026331745, -2.0530665, 1.9200616, -0.52789974, 0.5066756, -1.5408067, 0.8329863, 0.57878864, 1.0162872]                   |\n",
      "|[-0.85444444, 0.16009149, -1.5502977, 1.5754818, -0.514076, 0.8189558, -0.8028417, -0.17154066, 1.0645232, 0.71047723]                    |\n",
      "|[-1.1690692, 0.50356066, -0.97909236, 2.167345, -0.44589248, 0.6632595, -1.6676306, -0.5471983, 0.76977456, 1.5677092]                    |\n",
      "|[-0.16035773, 0.1484398, -1.294004, -0.13092956, -0.16831578, 0.17240724, -0.54272324, 0.4913819, 0.78555906, -0.112365395]               |\n",
      "|[-0.26051536, 0.29370168, -0.7328569, 1.1090214, -0.13335489, 0.10965943, -0.6426681, -0.044473313, 0.26237124, 0.25094542]               |\n",
      "|[-2.4961839, -0.49014783, -2.0337057, -0.75775045, -0.15425847, 0.9175591, -2.0849378, 2.4175193, -0.12628607, 0.5542613]                 |\n",
      "|[-0.11982112, 0.214738, -0.37779856, 0.2881665, -0.29000008, 0.3637495, -0.39518526, 0.05832204, 0.30171838, -0.24363661]                 |\n",
      "|[-0.03310038, -0.008717895, -0.035477955, 0.03029938, -5.7304966E-5, 0.031107094, 0.011791489, -0.033586726, 0.031981174, 0.034385335]    |\n",
      "|[-0.40877607, -0.21092053, -2.1736577, 0.8653673, -0.035617094, 0.210695, -1.2303308, 0.3111983, 1.0820992, 0.09498083]                   |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.userFactors.select(\"features\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "userID = 2093760\n",
    "\n",
    "existingArtistIDs = trainData.\\\n",
    "    filter(F.col(\"user\") == userID).rdd.\\\n",
    "    map(lambda row: int(row.artist)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "artistByID = buildArtistByID(rawArtistData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|     id|           name|\n",
      "+-------+---------------+\n",
      "|   1180|     David Gray|\n",
      "|    378|  Blackalicious|\n",
      "|    813|     Jurassic 5|\n",
      "|1255340|The Saw Doctors|\n",
      "|    942|         Xzibit|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artistByID.filter(F.col(\"id\").isin(existingArtistIDs)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "toRecommend = model.itemFactors.\\\n",
    "        select(F.col(\"id\").alias(\"artist\")).\\\n",
    "        withColumn(\"user\", F.lit(userID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeRecommendations(model, userID, howMany):\n",
    "    toRecommend = model.itemFactors.\\\n",
    "        select(F.col(\"id\").alias(\"artist\")).\\\n",
    "        withColumn(\"user\", F.lit(userID))\n",
    "    ans = model.transform(toRecommend).\\\n",
    "        select([\"artist\", \"prediction\"]).\\\n",
    "        orderBy(F.col(\"prediction\"), ascending=False).\\\n",
    "        limit(howMany)\n",
    "    return ans\n",
    "# end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "| artist| prediction|\n",
      "+-------+-----------+\n",
      "|   2814|0.029694578|\n",
      "|1001819|0.029153192|\n",
      "|1300642|0.029045615|\n",
      "|   4605|0.027700486|\n",
      "|1037970| 0.02766227|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topRecommendations = makeRecommendations(model, userID, 5)\n",
    "topRecommendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendedArtistIDs = topRecommendations.select(\"artist\").rdd.map(lambda row: int(row['artist'])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|     id|      name|\n",
      "+-------+----------+\n",
      "|   2814|   50 Cent|\n",
      "|   4605|Snoop Dogg|\n",
      "|   1811|   Dr. Dre|\n",
      "|1001819|      2Pac|\n",
      "|1300642|  The Game|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artistByID.filter(F.col(\"id\").isin(recommendedArtistIDs)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, features: array<float>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.userFactors.unpersist()\n",
    "model.itemFactors.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData = buildCounts(rawUserArtistData, bArtistAlias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, cvData = allData.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user: bigint, artist: bigint, count: bigint]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.cache()\n",
    "cvData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "allArtistIDs = allData.select(\"artist\").rdd.map(lambda row: int(row[\"artist\"])).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bAllArtistIDs = spark.sparkContext.broadcast(allArtistIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveData = cvData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictMostListened(train):\n",
    "    listenCounts = train.groupBy(\"artist\").\\\n",
    "        agg(F.sum(\"count\").alias(\"prediction\")).\\\n",
    "        select([\"artist\", \"prediction\"])\n",
    "    def func(allData):\n",
    "        return allData.\\\n",
    "          join(listenCounts, [\"artist\"], \"left_outer\").\\\n",
    "          select([\"user\", \"artist\", \"prediction\"])\n",
    "    # end def\n",
    "    return func\n",
    "# end def\n",
    "predictFunction = predictMostListened(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def areaUnderCurve(positiveData, bAllArtistIDs, predictFunction):\n",
    "    positivePredictions = predictFunction(positiveData.select([\"user\", \"artist\"])).\\\n",
    "    withColumnRenamed(\"prediction\", \"positivePrediction\")\n",
    "\n",
    "    # BinaryClassificationMetrics.areaUnderROC is not used here since there are really lots of\n",
    "    # small AUC problems, and it would be inefficient, when a direct computation is available.\n",
    "\n",
    "    # Create a set of \"negative\" products for each user. These are randomly chosen\n",
    "    # from among all of the other artists, excluding those that are \"positive\" for the user.\n",
    "    def func(item):\n",
    "        userID, userIDAndPosArtistIDs = item\n",
    "        posItemIDSet = list(userIDAndPosArtistIDs[1])\n",
    "        negative = []\n",
    "        allArtistIDs = bAllArtistIDs.value\n",
    "        i = 0\n",
    "        # Make at most one pass over all artists to avoid an infinite loop.\n",
    "        # Also stop when number of negative equals positive set size\n",
    "        while (i < len(allArtistIDs) and len(negative) < len(posItemIDSet)):\n",
    "            artistID = random.choice(allArtistIDs)\n",
    "            # Only add distinct IDs\n",
    "            if (artistID not in posItemIDSet):\n",
    "                negative.append(artistID)\n",
    "            # end def\n",
    "            i += 1\n",
    "        # end while\n",
    "        # Return the set with user ID added back\n",
    "        return (userID, artistID)\n",
    "    # end def\n",
    "\n",
    "    # BinaryClassificationMetrics.areaUnderROC is not used here since there are really lots of\n",
    "    # small AUC problems, and it would be inefficient, when a direct computation is available.\n",
    "\n",
    "    # Create a set of \"negative\" products for each user. These are randomly chosen\n",
    "    # from among all of the other artists, excluding those that are \"positive\" for the user.\n",
    "    negativeData = positiveData.select([\"user\", \"artist\"]).rdd.\\\n",
    "        map(lambda row: (int(row['user']), int(row['artist']))).\\\n",
    "        groupByKey().map(lambda item: (item[0], (item[0], item[1]))).\\\n",
    "        map(func).toDF([\"user\", \"artist\"])\n",
    "\n",
    "    # Make predictions on the rest:\n",
    "    negativePredictions = predictFunction(negativeData).\\\n",
    "        withColumnRenamed(\"prediction\", \"negativePrediction\")\n",
    "\n",
    "    # Join positive predictions to negative predictions by user, only.\n",
    "    # This will result in a row for every possible pairing of positive and negative\n",
    "    # predictions within each user.\n",
    "    joinedPredictions = positivePredictions.join(negativePredictions, \"user\").\\\n",
    "        select([\"user\", \"positivePrediction\", \"negativePrediction\"]).cache()\n",
    "\n",
    "    # Count the number of pairs per user\n",
    "    allCounts = joinedPredictions.\\\n",
    "        groupBy(\"user\").agg(F.count(F.lit(\"1\")).alias(\"total\")).\\\n",
    "        select([\"user\", \"total\"])\n",
    "    # Count the number of correctly ordered pairs per user\n",
    "    correctCounts = joinedPredictions.\\\n",
    "        filter(F.col(\"positivePrediction\") > F.col(\"negativePrediction\")).\\\n",
    "        groupBy(\"user\").agg(F.count(\"user\").alias(\"correct\")).\\\n",
    "        select([\"user\", \"correct\"])\n",
    "\n",
    "    # Combine these, compute their ratio, and average over all users\n",
    "    meanAUC = allCounts.join(correctCounts, [\"user\"], \"left_outer\").\\\n",
    "        select(\"user\", (F.coalesce(F.col(\"correct\"), F.lit(0)) / F.col(\"total\")).alias(\"auc\")).\\\n",
    "        agg(F.mean(\"auc\"))\n",
    "\n",
    "    meanAUC = meanAUC.collect()[0]\n",
    "    meanAUC = list(meanAUC.asDict().values())[0]\n",
    "    \n",
    "    joinedPredictions.unpersist()\n",
    "    return meanAUC\n",
    "# end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8782432847197721\n"
     ]
    }
   ],
   "source": [
    "mostListenedAUC = areaUnderCurve(cvData, bAllArtistIDs, predictMostListened(trainData))\n",
    "print(mostListenedAUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2522.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5154.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5154.0 (TID 10104, localhost, executor driver): java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:134)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:241)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1124)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1117)\n\tat org.apache.spark.ml.recommendation.ALS$.computeYtY(ALS.scala:1712)\n\tat org.apache.spark.ml.recommendation.ALS$.org$apache$spark$ml$recommendation$ALS$$computeFactors(ALS.scala:1653)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$train$4.apply(ALS.scala:983)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$train$4.apply(ALS.scala:970)\n\tat scala.collection.immutable.Range.foreach(Range.scala:160)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:970)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$fit$1.apply(ALS.scala:676)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$fit$1.apply(ALS.scala:658)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:658)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:569)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:134)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:241)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-8928615e648a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0msetUserCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetItemCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"artist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0msetRatingCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetPredictionCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mareaUnderCurve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbAllArtistIDs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PySpark-Analytics-Library/aas_python/ch03-recommender/venv/lib/python3.5/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/PySpark-Analytics-Library/aas_python/ch03-recommender/venv/lib/python3.5/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PySpark-Analytics-Library/aas_python/ch03-recommender/venv/lib/python3.5/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PySpark-Analytics-Library/aas_python/ch03-recommender/venv/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PySpark-Analytics-Library/aas_python/ch03-recommender/venv/lib/python3.5/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PySpark-Analytics-Library/aas_python/ch03-recommender/venv/lib/python3.5/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2522.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5154.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5154.0 (TID 10104, localhost, executor driver): java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:134)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:241)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1124)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1117)\n\tat org.apache.spark.ml.recommendation.ALS$.computeYtY(ALS.scala:1712)\n\tat org.apache.spark.ml.recommendation.ALS$.org$apache$spark$ml$recommendation$ALS$$computeFactors(ALS.scala:1653)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$train$4.apply(ALS.scala:983)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$train$4.apply(ALS.scala:970)\n\tat scala.collection.immutable.Range.foreach(Range.scala:160)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:970)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$fit$1.apply(ALS.scala:676)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$fit$1.apply(ALS.scala:658)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:658)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:569)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:134)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:241)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "evaluations = []\n",
    "for rank in (5, 30):\n",
    "    for regParam in (1.0, 0.0001):\n",
    "        for alpha in (1.0, 40.0):\n",
    "            model = ALS().\\\n",
    "                setSeed(random.randrange(0,10000000)).\\\n",
    "                setImplicitPrefs(True).\\\n",
    "                setRank(rank).setRegParam(regParam).\\\n",
    "                setAlpha(alpha).setMaxIter(20).\\\n",
    "                setUserCol(\"user\").setItemCol(\"artist\").\\\n",
    "                setRatingCol(\"count\").setPredictionCol(\"prediction\").\\\n",
    "                fit(trainData)\n",
    "\n",
    "            auc = areaUnderCurve(cvData, bAllArtistIDs, model.transform)\n",
    "\n",
    "            model.userFactors.unpersist()\n",
    "            model.itemFactors.unpersist()\n",
    "\n",
    "            ans = (auc, (rank, regParam, alpha))\n",
    "            evaluations.append(ans)\n",
    "        # end for\n",
    "    # end for\n",
    "# end for\n",
    "\n",
    "for _ in reversed(sorted(evaluations)):\n",
    "    print(_)\n",
    "# end for\n",
    "\n",
    "trainData.unpersist()\n",
    "cvData.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|       name|\n",
      "+-----------+\n",
      "|  Green Day|\n",
      "|  [unknown]|\n",
      "|The Beatles|\n",
      "|     Eminem|\n",
      "|         U2|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def recommend(rawUserArtistData, rawArtistData, rawArtistAlias):\n",
    "\n",
    "    bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\n",
    "    allData = buildCounts(rawUserArtistData, bArtistAlias).cache()\n",
    "    model = ALS().\\\n",
    "        setSeed(random.randrange(0,10000000)).\\\n",
    "        setImplicitPrefs(True).\\\n",
    "        setRank(10).setRegParam(1.0).setAlpha(40.0).setMaxIter(20).\\\n",
    "        setUserCol(\"user\").setItemCol(\"artist\").\\\n",
    "        setRatingCol(\"count\").setPredictionCol(\"prediction\").\\\n",
    "        fit(allData)\n",
    "    allData.unpersist()\n",
    "\n",
    "    userID = 2093760\n",
    "    topRecommendations = makeRecommendations(model, userID, 5)\n",
    "\n",
    "    recommendedArtistIDs = topRecommendations.select(\"artist\").rdd.map(lambda row: int(row[\"artist\"])).collect()\n",
    "    artistByID = buildArtistByID(rawArtistData)\n",
    "    artistByID.join(spark.createDataFrame(recommendedArtistIDs, IntegerType()).toDF(\"id\"), \"id\").\\\n",
    "        select(\"name\").show()\n",
    "\n",
    "    model.userFactors.unpersist()\n",
    "    model.itemFactors.unpersist()\n",
    "# end def\n",
    "recommend(rawUserArtistData, rawArtistData, rawArtistAlias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
